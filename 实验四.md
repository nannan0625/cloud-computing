实验四ceph安装

基本结构

4个节点：admin-node为部署节点，通过该节点向其他节点分发软件。mon.node1为监控节点（监控存储节点的情况），osd0，osd1为对象存储设备节点。 

实验拓扑



![tu1](/image4/tu1.png)

创建cephuser用户并且分配sudo权限

useradd -d /home/cephuser -m cephuser
passwd cephuser
echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser
sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers

![tu2](/image4/tu2.jpg)

安装ntp服务来同步各个电脑之间的时间

![tu3](/image4/tu3.png)

禁用SELINUX

![tu4](/image4/tu4.png)

添加ceph仓库

sudo vi /etc/yum.repos.d/ceph.repo

内容

[ceph]
name=Ceph packages for $basearch
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/$basearch
enabled=1
gpgcheck=0
priority=1
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch
enabled=1
gpgcheck=0
priority=1
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
[ceph-source]
name=Ceph source packages
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMS
enabled=0
gpgcheck=0
type=rpm-md
gpgkey=http://mirrors.163.com/ceph/keys/release.asc
priority=1

![tu5](/image4/tu5.png)

yum clean all
curl http://mirrors.aliyun.com/repo/Centos-7.repo >/etc/yum.repos.d/CentOS-Base.repo
curl http://mirrors.aliyun.com/repo/epel-7.repo >/etc/yum.repos.d/epel.repo 
sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo
sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo
yum makecache

防火墙设置

![tu6](/image4/tu6.png)

基础配置完完整克隆三个虚拟机分别为mon1，osd1，osd2

![tu7](/image4/tu7.png)

主控节点设置防火墙

sudo firewall-cmd --zone=public --add-port=80/tcp --permanent
sudo firewall-cmd --zone=public --add-port=2003/tcp --permanent
sudo firewall-cmd --zone=public --add-port=4505-4506/tcp --permanent
sudo firewall-cmd --reload

![tu8](/image4/tu8.png)

mon1设置防火墙

sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent
sudo firewall-cmd --reload

![tu9](/image4/tu9.png)

osd1和osd2设置防火墙

sudo firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent
sudo firewall-cmd --reload  

![tu10](/image4/tu10.png)

![tu11](/image4/tu11.png)

配置ssh免密登陆

sudo vi /etc/hosts

![tu12](/image4/tu12.png)

su - cephuser

ssh-keygen

![tu13](/image4/tu13.png)

sudo vi ~/.ssh/config

![tu14](/image4/tu14.png)

设置文件权限

chmod 644 ~/.ssh/config
ssh-keyscan osd1 osd2 mon1 >> ~/.ssh/known_hosts

把密钥发送到各个节点

ssh-copy-id ceph-admin
ssh-copy-id mon1
ssh-copy-id osd1
ssh-copy-id osd2

测试ssh登陆各个节点

![tu15](/image4/tu15.png)

安装ceph部署工具（ceph-deploy）

sudo yum install ceph-deploy

![tu16](/image4/tu16.png)

创建集群

mkdir cluster && cd cluster
ceph-deploy new mon1
vi ceph.conf

![tu17](/image4/tu17.png)

![tu18](/image4/tu18.png)

使用ceph-deploy从主控节点在各个节点上安装ceph

ceph-deploy install ceph-admin mon1 osd1 osd2

![tu19](/image4/tu19.png)

查看版本ceph -v

![20](/image4/tu20.png)

修改监控节点名字并添加进开机设置

![tu21](/image4/tu21.png)

配置集群

主控节点

ceph-deploy mon create-initial

![tu22](/image4/tu22.jpg)

ceph-deploy gatherkeys mon1

![tu23](/image4/tu23.jpg)

osd节点

sudo mkdir /var/local/osd
sudo chown ceph: /var/local/osd

![tu24](/image4/tu24.png)

![tu25](/image4/tu25.png)

主控节点

将管理密钥部署到所有关联的节点。

准备所有OSDS节点

ceph-deploy osd prepare osd1:/var/local/osd osd2:/var/local/osd

激活OSD

ceph-deploy osd activate osd1:/var/local/osd osd2:/var/local/osd

![tu26](/image4/tu26.jpg)

将管理密钥部署到所有关联的节点。

ceph-deploy admin ceph-admin mon1 osd1 osd2

![tu27](/image4/tu27.png)

通过在所有节点上运行以下命令来更改密钥文件的权限

sudo chmod 644 /etc/ceph/ceph.client.admin.keyring

![tu28](/image4/tu28.png)

在mon1查看集群运行的情况sudo ceph health

![tu29](/image4/tu29.png)

查看集群状态sudo ceph -s

![tu30](/image4/tu30.png)